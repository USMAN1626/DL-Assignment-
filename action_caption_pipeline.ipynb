{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b038adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, random, pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "import uvicorn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e465d31",
   "metadata": {},
   "source": [
    "## CNN Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e1a82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/muhammadusman/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:14<00:00, 7.04MB/s]\n",
      "100%|██████████| 8091/8091 [03:21<00:00, 40.12it/s]\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"dataset/archive-2/Images\"\n",
    "feature_store = \"artifacts/resnet50_features.pt\"\n",
    "os.makedirs(os.path.dirname(feature_store), exist_ok=True)\n",
    "cnn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "cnn_fc_dim = cnn.fc.in_features\n",
    "cnn.fc = nn.Identity()\n",
    "cnn = cnn.to(device).eval()\n",
    "feat_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def extract_features(dir_path, out_path):\n",
    "    paths = [str(p) for p in pathlib.Path(dir_path).rglob('*.jpg')] + [str(p) for p in pathlib.Path(dir_path).rglob('*.png')]\n",
    "    feats = {}\n",
    "    with torch.no_grad():\n",
    "        for p in tqdm(paths):\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            t = feat_transform(img).unsqueeze(0).to(device)\n",
    "            f = cnn(t).squeeze(0).cpu()\n",
    "            feats[os.path.basename(p)] = f\n",
    "    torch.save({\"features\": feats, \"dim\": cnn_fc_dim}, out_path)\n",
    "extract_features(image_dir, feature_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6d488",
   "metadata": {},
   "source": [
    "## Flickr8k Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a5820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images at: /Users/muhammadusman/Desktop/dl/dataset/archive-2/Images\n",
      "Directory exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1265 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (640) to match target batch_size (608).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m     feats=cnn(imgs)\n\u001b[32m    121\u001b[39m logits=caption_model(feats, caps)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m loss=\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m opt.zero_grad()\n\u001b[32m    124\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl/.venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl/.venv/lib/python3.14/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/dl/.venv/lib/python3.14/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (640) to match target batch_size (608)."
     ]
    }
   ],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "dataset_root = os.path.join(notebook_dir, \"dataset\", \"archive-2\")\n",
    "flickr_root = dataset_root\n",
    "flickr_images = os.path.join(flickr_root, \"Images\")\n",
    "flickr_captions = os.path.join(flickr_root, \"captions.txt\")\n",
    "\n",
    "# Debug: print the paths to verify\n",
    "print(f\"Looking for images at: {flickr_images}\")\n",
    "print(f\"Directory exists: {os.path.isdir(flickr_images)}\")\n",
    "\n",
    "# quick path checks to surface path issues early\n",
    "if not os.path.isdir(flickr_images):\n",
    "    raise FileNotFoundError(f\"Image folder not found at {flickr_images}\")\n",
    "if not os.path.isfile(flickr_captions):\n",
    "    raise FileNotFoundError(f\"Captions file not found at {flickr_captions}\")\n",
    "min_freq = 2\n",
    "max_len = 20\n",
    "pad_token, start_token, end_token, unk_token = \"<pad>\", \"<start>\", \"<end>\", \"<unk>\"\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.freqs = {}\n",
    "        self.stoi = {pad_token:0, start_token:1, end_token:2, unk_token:3}\n",
    "        self.itos = [pad_token, start_token, end_token, unk_token]\n",
    "    def build(self, lines):\n",
    "        for line in lines:\n",
    "            for w in line.split():\n",
    "                self.freqs[w] = self.freqs.get(w,0)+1\n",
    "        for w,f in self.freqs.items():\n",
    "            if f>=self.min_freq and w not in self.stoi:\n",
    "                self.stoi[w]=len(self.itos)\n",
    "                self.itos.append(w)\n",
    "    def encode(self, text):\n",
    "        tokens=[start_token]+[w if w in self.stoi else unk_token for w in text.split()][:max_len-2]+[end_token]\n",
    "        ids=[self.stoi.get(t, self.stoi[unk_token]) for t in tokens]\n",
    "        if len(ids)<max_len:\n",
    "            ids+= [self.stoi[pad_token]]*(max_len-len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    def decode(self, ids):\n",
    "        words=[]\n",
    "        for i in ids:\n",
    "            w=self.itos[i]\n",
    "            if w==end_token:\n",
    "                break\n",
    "            if w not in {start_token,pad_token}:\n",
    "                words.append(w)\n",
    "        return \" \".join(words)\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, vocab, transform):\n",
    "        with open(caption_file) as f:\n",
    "            lines = [line.strip() for line in f if len(line.strip())>0]\n",
    "            # Skip header line\n",
    "            raw=[line.split(',', 1) for line in lines[1:]]\n",
    "        self.data=[(parts[0], parts[1]) for parts in raw if len(parts)==2]\n",
    "        vocab.build([c for _,c in self.data])\n",
    "        self.vocab=vocab\n",
    "        self.dir=img_dir\n",
    "        self.transform=transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        fname,cap=self.data[idx]\n",
    "        path=os.path.join(self.dir,fname)\n",
    "        img=Image.open(path).convert('RGB')\n",
    "        img=self.transform(img)\n",
    "        cap_ids=self.vocab.encode(cap)\n",
    "        return img, cap_ids\n",
    "caption_transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "vocab=Vocabulary(min_freq=min_freq)\n",
    "train_ds=Flickr8kDataset(flickr_images, flickr_captions, vocab, caption_transform)\n",
    "def collate(batch):\n",
    "    imgs=torch.stack([b[0] for b in batch])\n",
    "    caps=torch.stack([b[1] for b in batch])\n",
    "    return imgs,caps\n",
    "train_loader=DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate, num_workers=0)\n",
    "vocab_size=len(vocab.itos)\n",
    "embed_dim=256\n",
    "hidden_dim=512\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm=nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.init_fc=nn.Linear(feature_dim, embed_dim)\n",
    "        self.fc=nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, features, captions):\n",
    "        emb=self.embed(captions[:,:-1])\n",
    "        init=self.init_fc(features).unsqueeze(1)\n",
    "        inputs=torch.cat([init, emb], dim=1)\n",
    "        out,_=self.lstm(inputs)\n",
    "        logits=self.fc(out)\n",
    "        return logits\n",
    "    def generate(self, feature, max_len=20):\n",
    "        seq=[1]\n",
    "        hidden=None\n",
    "        x=self.init_fc(feature).unsqueeze(0).unsqueeze(1)\n",
    "        for _ in range(max_len):\n",
    "            out,hidden=self.lstm(x,hidden)\n",
    "            logits=self.fc(out[:, -1])\n",
    "            next_id=logits.argmax(dim=-1).item()\n",
    "            seq.append(next_id)\n",
    "            if next_id==2:\n",
    "                break\n",
    "            x=self.embed(torch.tensor([[next_id]], device=feature.device))\n",
    "        return seq\n",
    "caption_model=CaptionModel(embed_dim, hidden_dim, vocab_size, cnn_fc_dim).to(device)\n",
    "opt=optim.Adam(caption_model.parameters(), lr=1e-4)\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=0)\n",
    "caption_epochs=5\n",
    "for epoch in range(caption_epochs):\n",
    "    caption_model.train()\n",
    "    total_loss=0.0\n",
    "    for imgs,caps in tqdm(train_loader):\n",
    "        imgs,caps=imgs.to(device),caps.to(device)\n",
    "        with torch.no_grad():\n",
    "            feats=cnn(imgs)\n",
    "        logits=caption_model(feats, caps)\n",
    "        loss=criterion(logits[:,1:,:].reshape(-1, vocab_size), caps[:,1:].reshape(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss+=loss.item()\n",
    "    torch.save({\"model\":caption_model.state_dict(), \"vocab\":vocab.itos}, f\"artifacts/caption_epoch_{epoch+1}.pt\")\n",
    "    print(f\"epoch {epoch+1} loss {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c88f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_model.eval()\n",
    "sample_path=os.path.join(flickr_images, os.listdir(flickr_images)[0])\n",
    "img=Image.open(sample_path).convert('RGB')\n",
    "t=caption_transform(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    f=cnn(t)\n",
    "    ids=caption_model.generate(f.squeeze(0))\n",
    "text=vocab.decode(ids[1:])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94cb29c",
   "metadata": {},
   "source": [
    "## Stanford Actions Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_root=\"data/stanford_actions\"\n",
    "train_dir=os.path.join(stanford_root, \"train\")\n",
    "val_dir=os.path.join(stanford_root, \"val\")\n",
    "action_transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "train_ds_act=datasets.ImageFolder(train_dir, transform=action_transform)\n",
    "val_ds_act=datasets.ImageFolder(val_dir, transform=action_transform)\n",
    "train_loader_act=DataLoader(train_ds_act, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader_act=DataLoader(val_ds_act, batch_size=32, shuffle=False, num_workers=2)\n",
    "num_classes=len(train_ds_act.classes)\n",
    "action_model=models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "action_model.fc=nn.Linear(action_model.fc.in_features, num_classes)\n",
    "action_model=action_model.to(device)\n",
    "opt_act=optim.Adam(action_model.parameters(), lr=3e-4)\n",
    "criterion_act=nn.CrossEntropyLoss()\n",
    "act_epochs=5\n",
    "for epoch in range(act_epochs):\n",
    "    action_model.train()\n",
    "    train_loss=0.0\n",
    "    for imgs,labels in tqdm(train_loader_act):\n",
    "        imgs,labels=imgs.to(device),labels.to(device)\n",
    "        logits=action_model(imgs)\n",
    "        loss=criterion_act(logits, labels)\n",
    "        opt_act.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_act.step()\n",
    "        train_loss+=loss.item()\n",
    "    action_model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for imgs,labels in val_loader_act:\n",
    "            imgs,labels=imgs.to(device),labels.to(device)\n",
    "            preds=action_model(imgs).argmax(dim=1)\n",
    "            correct+= (preds==labels).sum().item()\n",
    "            total+= labels.numel()\n",
    "    acc=correct/total if total>0 else 0.0\n",
    "    torch.save(action_model.state_dict(), f\"artifacts/action_epoch_{epoch+1}.pt\")\n",
    "    print(f\"epoch {epoch+1} loss {train_loss/len(train_loader_act):.4f} val_acc {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5ad8a",
   "metadata": {},
   "source": [
    "## FastAPI Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_ckpt=\"artifacts/caption_epoch_5.pt\"\n",
    "action_ckpt=\"artifacts/action_epoch_5.pt\"\n",
    "cnn.eval()\n",
    "caption_model.eval()\n",
    "action_model.eval()\n",
    "if os.path.exists(caption_ckpt):\n",
    "    payload=torch.load(caption_ckpt, map_location=device)\n",
    "    caption_model.load_state_dict(payload[\"model\"])\n",
    "    vocab.itos=payload[\"vocab\"]\n",
    "    vocab.stoi={w:i for i,w in enumerate(vocab.itos)}\n",
    "if os.path.exists(action_ckpt):\n",
    "    action_model.load_state_dict(torch.load(action_ckpt, map_location=device))\n",
    "app=FastAPI()\n",
    "def predict_caption(img_tensor):\n",
    "    with torch.no_grad():\n",
    "        f=cnn(img_tensor).squeeze(0)\n",
    "        ids=caption_model.generate(f)\n",
    "        return vocab.decode(ids[1:])\n",
    "def predict_action(img_tensor):\n",
    "    with torch.no_grad():\n",
    "        logits=action_model(img_tensor)\n",
    "        idx=logits.argmax(dim=1).item()\n",
    "        return train_ds_act.classes[idx]\n",
    "def load_image(file_bytes):\n",
    "    img=Image.open(io.BytesIO(file_bytes)).convert('RGB')\n",
    "    t=caption_transform(img).unsqueeze(0).to(device)\n",
    "    t_act=action_transform(img).unsqueeze(0).to(device)\n",
    "    return t, t_act\n",
    "@app.post('/predict')\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    data=await file.read()\n",
    "    t, t_act=load_image(data)\n",
    "    cap=predict_caption(t)\n",
    "    act=predict_action(t_act)\n",
    "    return {\"caption\": cap, \"action\": act}\n",
    "# if __name__ == '__main__':\n",
    "#     uvicorn.run(app, host='0.0.0.0', port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
